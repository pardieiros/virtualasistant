# Implementa√ß√£o de Streaming SSE para Chat

Este documento descreve a implementa√ß√£o completa do sistema de streaming de respostas do Ollama usando Server-Sent Events (SSE).

## üìã Resumo

Implement√°mos um sistema de streaming que permite ao frontend receber as respostas do LLM incrementalmente, token por token, melhorando significativamente a experi√™ncia do utilizador.

### Melhorias de Performance

1. **Caching Inteligente:**
   - Base system prompt: cache de 1 hora
   - Contexto do utilizador (HA devices/aliases): cache de 10 minutos
   - Mem√≥rias relevantes: cache de 60 segundos com heur√≠stica de keywords
   
2. **Otimiza√ß√µes:**
   - Hist√≥rico limitado √†s √∫ltimas 12 mensagens
   - Memory search apenas quando relevante (heur√≠stica)
   - HA states n√£o s√£o chamados no system prompt (s√≥ quando necess√°rio)

3. **Streaming:**
   - Respostas come√ßam a aparecer imediatamente
   - Chunks enviados assim que chegam do Ollama
   - ACTION parsing no final, n√£o enviada ao UI

## üèóÔ∏è Arquitetura

### Backend (Django)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Client (Browser)                               ‚îÇ
‚îÇ  ‚Üì                                              ‚îÇ
‚îÇ  POST /api/chat/stream/                        ‚îÇ
‚îÇ  ‚Üì                                              ‚îÇ
‚îÇ  ChatStreamView (views.py)                     ‚îÇ
‚îÇ  ‚Üì                                              ‚îÇ
‚îÇ  build_messages() + stream_ollama_chat()       ‚îÇ
‚îÇ  ‚Üì                                              ‚îÇ
‚îÇ  Ollama API (stream=True)                      ‚îÇ
‚îÇ  ‚Üì                                              ‚îÇ
‚îÇ  SSE Events ‚Üí Client                           ‚îÇ
‚îÇ  ‚Ä¢ event: message (chunks de texto)            ‚îÇ
‚îÇ  ‚Ä¢ event: final_text (texto limpo sem ACTION)  ‚îÇ
‚îÇ  ‚Ä¢ event: action (ACTION detectada)            ‚îÇ
‚îÇ  ‚Ä¢ event: done (stream completo)               ‚îÇ
‚îÇ  ‚Ä¢ event: error (erro)                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Frontend (React)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  StreamingChat.tsx                      ‚îÇ
‚îÇ  ‚Üì                                      ‚îÇ
‚îÇ  useChatStream() hook                  ‚îÇ
‚îÇ  ‚Üì                                      ‚îÇ
‚îÇ  fetch() com ReadableStream            ‚îÇ
‚îÇ  ‚Üì                                      ‚îÇ
‚îÇ  Parse SSE events                      ‚îÇ
‚îÇ  ‚Üì                                      ‚îÇ
‚îÇ  Update UI incrementalmente            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìÅ Ficheiros Criados/Modificados

### Backend

1. **`backend/assistant/services/prompt_cache.py`** (NOVO)
   - Sistema de caching para prompts e contexto
   - Fun√ß√µes: `get_base_system_prompt_cached()`, `get_user_context_cached()`, `get_relevant_memories_cached()`
   - Cache keys e TTLs configur√°veis

2. **`backend/assistant/services/ollama_client.py`** (MODIFICADO)
   - Adicionadas fun√ß√µes:
     - `get_base_system_prompt()` - prompt base est√°tico
     - `get_time_prompt()` - informa√ß√£o temporal
     - `get_user_context_prompt()` - contexto do utilizador
     - `stream_ollama_chat()` - streaming do Ollama com SSE
   - Modificadas fun√ß√µes:
     - `get_system_prompt()` - agora usa cache
     - `build_messages()` - usa cache e limita hist√≥rico

3. **`backend/assistant/views.py`** (MODIFICADO)
   - Nova classe `ChatStreamView` com endpoints GET e POST
   - SSE events: chunk, done, action, error, final_text
   - Headers corretos para SSE

4. **`backend/assistant/urls.py`** (MODIFICADO)
   - Nova rota: `path('chat/stream/', ChatStreamView.as_view(), name='chat_stream')`

### Frontend

5. **`frontend/src/hooks/useChatStream.ts`** (NOVO)
   - Hook React para consumir SSE
   - Gest√£o de estado: messages, isStreaming, error, action
   - Suporte para cancelamento de stream

6. **`frontend/src/components/StreamingChat.tsx`** (NOVO)
   - Componente de exemplo com UI completa
   - Typing indicator animado
   - Display de erros e actions
   - Auto-scroll

### Documenta√ß√£o

7. **`SSE_NGINX_CONFIG.md`** (NOVO)
   - Configura√ß√µes Nginx necess√°rias
   - Troubleshooting
   - Exemplos completos

8. **`STREAMING_IMPLEMENTATION.md`** (NOVO - este ficheiro)
   - Documenta√ß√£o completa da implementa√ß√£o

## üöÄ Como Usar

### 1. Backend - Endpoint SSE

#### POST Request (recomendado)

```python
# JavaScript/TypeScript
const response = await fetch('/api/chat/stream/', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
    'Authorization': `Bearer ${token}`,
  },
  body: JSON.stringify({
    message: 'Ol√°, como est√°s?',
    history: [
      { role: 'user', content: 'Mensagem anterior' },
      { role: 'assistant', content: 'Resposta anterior' }
    ],
    conversation_id: 123  // opcional
  }),
});

const reader = response.body.getReader();
// ... processar stream
```

#### GET Request (simples)

```bash
curl -N -H "Authorization: Bearer TOKEN" \
  "http://localhost:8000/api/chat/stream/?message=Ol√°"
```

### 2. Frontend - useChatStream Hook

```tsx
import { useChatStream } from '../hooks/useChatStream';

function MyChat() {
  const {
    sendMessage,
    messages,
    isStreaming,
    error,
    action,
    cancelStream,
    currentStreamingMessage,
  } = useChatStream();

  const handleSend = async () => {
    await sendMessage('Qual o estado do tempo?', messages);
  };

  return (
    <div>
      {messages.map((msg, i) => (
        <div key={i}>{msg.content}</div>
      ))}
      
      {isStreaming && currentStreamingMessage && (
        <div>{currentStreamingMessage}<span className="animate-pulse">|</span></div>
      )}
      
      {isStreaming && (
        <button onClick={cancelStream}>Cancelar</button>
      )}
    </div>
  );
}
```

### 3. Nginx - Configura√ß√£o

Edita `nginx/nginx.conf`:

```nginx
location /api/chat/stream/ {
    proxy_pass http://backend:8000;
    
    # CR√çTICO para SSE
    proxy_buffering off;
    proxy_cache off;
    proxy_set_header X-Accel-Buffering no;
    
    # Timeouts longos
    proxy_read_timeout 300s;
    proxy_connect_timeout 75s;
    
    # Headers
    proxy_http_version 1.1;
    proxy_set_header Connection '';
    add_header Cache-Control 'no-cache';
}
```

## üìä Formato dos Eventos SSE

### Event: message (chunk)

```
data: {"type": "chunk", "content": "Ol√° "}

data: {"type": "chunk", "content": "como "}

data: {"type": "chunk", "content": "est√°s?"}
```

### Event: final_text

Enviado quando h√° ACTION no texto (texto limpo sem linha ACTION):

```
event: final_text
data: {"text": "Vou ligar o ar condicionado da sala a 22 graus."}
```

### Event: action

Enviado quando ACTION √© detectada:

```
event: action
data: {"action": {"tool": "homeassistant_call_service", "args": {...}}}
```

### Event: done

Sinaliza fim do stream:

```
event: done
data: {"finished": true}
```

### Event: error

Enviado em caso de erro:

```
event: error
data: {"error": "Ollama connection failed"}
```

## üîß Configura√ß√£o do Cache

### Django Settings

Adiciona ao `settings.py` se ainda n√£o existir:

```python
CACHES = {
    'default': {
        'BACKEND': 'django.core.cache.backends.locmem.LocMemCache',
        'LOCATION': 'unique-snowflake',
        'OPTIONS': {
            'MAX_ENTRIES': 1000
        }
    }
}
```

Para produ√ß√£o, considera usar Redis:

```python
CACHES = {
    'default': {
        'BACKEND': 'django.core.cache.backends.redis.RedisCache',
        'LOCATION': 'redis://redis:6379/1',
        'OPTIONS': {
            'CLIENT_CLASS': 'django_redis.client.DefaultClient',
        },
        'KEY_PREFIX': 'jarvas'
    }
}
```

## üß™ Testes

### 1. Testar Backend SSE

```bash
# Terminal 1 - Start services
cd /opt/virtualasistant
docker-compose up

# Terminal 2 - Test endpoint
curl -N -X POST http://localhost/api/chat/stream/ \
  -H "Authorization: Bearer YOUR_TOKEN" \
  -H "Content-Type: application/json" \
  -d '{"message": "Ol√°, como est√°s?"}'
```

Deves ver chunks a chegar incrementalmente.

### 2. Testar Frontend

```bash
cd /opt/virtualasistant/frontend
npm run dev
```

Abre `http://localhost:5173` e testa o componente StreamingChat.

### 3. Testar Cache

```python
# Django shell
python manage.py shell

from assistant.services.prompt_cache import *
from django.contrib.auth.models import User

user = User.objects.first()

# Test 1: Base prompt (deve ser r√°pido ap√≥s 1¬™ chamada)
prompt1 = get_base_system_prompt_cached()
prompt2 = get_base_system_prompt_cached()  # Cache hit

# Test 2: User context (10 min cache)
ctx1 = get_user_context_cached(user)
ctx2 = get_user_context_cached(user)  # Cache hit

# Test 3: Memories (com heur√≠stica)
mems1 = get_relevant_memories_cached(user, "lembras-te do meu anivers√°rio?")
mems2 = get_relevant_memories_cached(user, "lembras-te do meu anivers√°rio?")  # Cache hit
mems3 = get_relevant_memories_cached(user, "que horas s√£o?")  # Nenhuma (heur√≠stica)
```

## üêõ Troubleshooting

### Problema: Stream n√£o funciona (tudo chega de uma vez)

**Causa:** Nginx est√° a bufferizar.

**Solu√ß√£o:**
1. Verifica `nginx.conf` ‚Üí `proxy_buffering off;`
2. Reinicia Nginx: `docker restart virtualasistant_nginx`
3. Verifica response headers no DevTools

### Problema: Timeout ap√≥s alguns segundos

**Causa:** Timeouts muito curtos.

**Solu√ß√£o:**
1. Aumenta `proxy_read_timeout` no Nginx
2. Aumenta timeout do Gunicorn: `--timeout 300`

### Problema: Chunks duplicados no UI

**Causa:** Hook est√° a acumular chunks incorretamente.

**Solu√ß√£o:**
- Verifica que `currentStreamingMessage` √© resetado em cada nova mensagem
- Usa key √∫nica para cada mensagem no render

### Problema: ACTION aparece no UI

**Causa:** Frontend n√£o est√° a tratar evento `final_text`.

**Solu√ß√£o:**
- Implementa handler para `event: final_text` no hook
- Substitui `currentStreamingMessage` com texto limpo

## üìà Performance Esperada

### Antes (sem streaming)

- Tempo at√© primeira resposta: **5-15 segundos**
- Utilizador v√™: loading spinner
- Backend: chama HA states, mem√≥rias, sem cache

### Depois (com streaming + cache)

- Tempo at√© primeiro chunk: **0.5-2 segundos**
- Utilizador v√™: resposta a aparecer token por token
- Backend: usa cache, skip desnecess√°rio

### M√©tricas

```
Cache Hit Rate:
- Base prompt: ~99% (raramente muda)
- User context: ~95% (10 min cache)
- Memories: ~30% (muitas queries √∫nicas)

Latency Improvement:
- First chunk: 5-10x mais r√°pido
- Perceived performance: melhoria significativa
```

## üîê Seguran√ßa

1. **Authentication:** Todos os endpoints requerem `IsAuthenticated`
2. **User Isolation:** Queries filtradas por `request.user`
3. **Rate Limiting:** Considera adicionar rate limiting para SSE
4. **Resource Limits:** Stream auto-fecha ap√≥s timeout configurado

## üöÄ Pr√≥ximos Passos (Opcional)

1. **WebSocket Alternative:** Para comunica√ß√£o bidirecional
2. **Retry Logic:** Auto-reconnect no frontend se stream falhar
3. **Progress Indicators:** Mostrar % de tokens gerados
4. **Action Execution:** Executar actions automaticamente no frontend
5. **Voice Output:** TTS incremental dos chunks

## üìö Refer√™ncias

- [MDN: Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)
- [Django StreamingHttpResponse](https://docs.djangoproject.com/en/stable/ref/request-response/#streaminghttpresponse-objects)
- [Ollama API Streaming](https://github.com/ollama/ollama/blob/main/docs/api.md#generate-a-chat-completion)
- [Nginx Proxy Buffering](http://nginx.org/en/docs/http/ngx_http_proxy_module.html#proxy_buffering)

---

**Implementado por:** AI Assistant  
**Data:** 2025-12-18  
**Vers√£o:** 1.0
















